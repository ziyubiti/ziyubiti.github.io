<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>BP算法摘要与代码分析 | ziyubiti</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/4.2.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.0.0/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">BP算法摘要与代码分析</h1><a id="logo" href="/.">ziyubiti</a><p class="description">study hard everyday</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> Home</i></a><a href="/categories/ML/"><i class="fa undefined"> ML</i></a><a href="/categories/Python/"><i class="fa undefined"> Python</i></a><a href="/categories/Web/"><i class="fa undefined"> Web</i></a><a href="/categories/5G/"><i class="fa undefined"> 5G</i></a><a href="/categories/Funny/"><i class="fa undefined"> Funny</i></a><a href="/archives/"><i class="fa undefined"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">BP算法摘要与代码分析</h1><div class="post-meta">Sep 18, 2016<span> | </span><span class="category"><a href="/categories/ML/">ML</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a data-thread-key="2016/09/18/BPbrief/" href="/2016/09/18/BPbrief/#comments" class="ds-thread-count"></a><div class="post-content"><p>　　反向传播算法是神经网络中的重要组成部分，是对权重和偏置变化影响代价函数过程的理解，目标是计算代价函数C 分别关于w 和b的偏导数，从而更新w、b，影响前向过程，最小化代价函数。<br>　　<img src="../../../../imgs/BP/0.png" alt="神经网络结构示意图"><br>　　<img src="../../../../imgs/BP/1.png" alt="BP算法的4个方程"><br>　　公式BP1与BP2在nielsen的书<a href="http://neuralnetworksanddeeplearning.com/chap2.html" target="_blank" rel="external">Neural Networks and Deep Learning</a>中已有证明，下面补充BP3与BP4的证明：<br>　　<img src="../../../../imgs/BP/2.png" alt="BP3 and BP4 Proof"><br>　　BP算法过程如下：<br>　　<img src="../../../../imgs/BP/3.png" alt="BP算法过程"><br>　　在m大小的mini_batch内进行一次梯度更新，过程如下：<br>　　<img src="../../../../imgs/BP/4.png" alt="mini_batch内的一次梯度更新">    　　<br>　　<br>　　以mnist识别，三层网络[784,30,10]为例，nielsen的书中代码如下，添加了部分注释：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_mini_batch</span><span class="params">(self, mini_batch, eta)</span>:</span></div><div class="line"></div><div class="line">        <span class="string">"""Update the network's weights and biases by applying</span></div><div class="line"></div><div class="line">        gradient descent using backpropagation to a single mini batch.</div><div class="line"></div><div class="line">        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``</div><div class="line"></div><div class="line">        is the learning rate."""</div><div class="line"></div><div class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]    <span class="comment"># idx0:30*1; idx1:10*1</span></div><div class="line"></div><div class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]   <span class="comment"># idx0:30*784; idx1:10*30</span></div><div class="line"></div><div class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> mini_batch:                              <span class="comment"># loop 10, BP calc  and update gradient 10 times</span></div><div class="line"></div><div class="line">            delta_nabla_b, delta_nabla_w = self.backprop(x, y)        <span class="comment"># delta b,delta w , their dimension is same as b,w</span></div><div class="line"></div><div class="line">            nabla_b = [nb+dnb <span class="keyword">for</span> nb, dnb <span class="keyword">in</span> zip(nabla_b, delta_nabla_b)]  <span class="comment"># bj:zip two column vector, but list output row by row</span></div><div class="line"></div><div class="line">            nabla_w = [nw+dnw <span class="keyword">for</span> nw, dnw <span class="keyword">in</span> zip(nabla_w, delta_nabla_w)]</div><div class="line"></div><div class="line">        self.weights = [w-(eta/len(mini_batch))*nw</div><div class="line"></div><div class="line">                        <span class="keyword">for</span> w, nw <span class="keyword">in</span> zip(self.weights, nabla_w)]     <span class="comment"># update w, sum the average 10 changes</span></div><div class="line"></div><div class="line">        self.biases = [b-(eta/len(mini_batch))*nb</div><div class="line"></div><div class="line">                       <span class="keyword">for</span> b, nb <span class="keyword">in</span> zip(self.biases, nabla_b)]       <span class="comment"># update b,</span></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backprop</span><span class="params">(self, x, y)</span>:</span></div><div class="line"></div><div class="line">        <span class="string">"""Return a tuple ``(nabla_b, nabla_w)`` representing the</span></div><div class="line"></div><div class="line">        gradient for the cost function C_x.  ``nabla_b`` and</div><div class="line"></div><div class="line">        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar</div><div class="line"></div><div class="line">        to ``self.biases`` and ``self.weights``."""</div><div class="line"></div><div class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</div><div class="line"></div><div class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</div><div class="line"></div><div class="line">        <span class="comment"># feedforward</span></div><div class="line"></div><div class="line">        activation = x</div><div class="line"></div><div class="line">        activations = [x] <span class="comment"># list to store all the activations, layer by layer</span></div><div class="line"></div><div class="line">        zs = [] <span class="comment"># list to store all the z vectors, layer by layer</span></div><div class="line"></div><div class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</div><div class="line"></div><div class="line">            z = np.dot(w, activation)+b</div><div class="line"></div><div class="line">            zs.append(z)                              <span class="comment"># middle variable z before sigmoid, z1,z2</span></div><div class="line"></div><div class="line">            activation = sigmoid(z)</div><div class="line"></div><div class="line">            activations.append(activation)            <span class="comment"># activation output, x, s1,s2</span></div><div class="line"></div><div class="line">        <span class="comment"># backward pass</span></div><div class="line"></div><div class="line">        delta = self.cost_derivative(activations[<span class="number">-1</span>], y) * \                <span class="comment"># C=1/2 *(s2-y).^2, so dc/ds=(s2-y)</span></div><div class="line"></div><div class="line">            sigmoid_prime(zs[<span class="number">-1</span>])                                            <span class="comment"># delta 10*1</span></div><div class="line"></div><div class="line">        nabla_b[<span class="number">-1</span>] = delta</div><div class="line"></div><div class="line">        nabla_w[<span class="number">-1</span>] = np.dot(delta, activations[<span class="number">-2</span>].transpose())           <span class="comment"># s1: 30*1, s1':1*30; delta:10*1, dot:10*30</span></div><div class="line"></div><div class="line">        <span class="comment"># Note that the variable l in the loop below is used a little</span></div><div class="line"></div><div class="line">        <span class="comment"># differently to the notation in Chapter 2 of the book.  Here,</span></div><div class="line"></div><div class="line">        <span class="comment"># l = 1 means the last layer of neurons, l = 2 is the</span></div><div class="line"></div><div class="line">        <span class="comment"># second-last layer, and so on.  It's a renumbering of the</span></div><div class="line"></div><div class="line">        <span class="comment"># scheme in the book, used here to take advantage of the fact</span></div><div class="line"></div><div class="line">        <span class="comment"># that Python can use negative indices in lists.</span></div><div class="line"></div><div class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> xrange(<span class="number">2</span>, self.num_layers):</div><div class="line"></div><div class="line">            z = zs[-l]                                                         </div><div class="line"></div><div class="line">            sp = sigmoid_prime(z)                                           <span class="comment">#sp1: dz1, 30*1</span></div><div class="line"></div><div class="line">            delta = np.dot(self.weights[-l+<span class="number">1</span>].transpose(), delta) * sp       <span class="comment"># w[1]: 10*30;  delta update to 30*1 </span></div><div class="line"></div><div class="line">            nabla_b[-l] = delta</div><div class="line"></div><div class="line">            nabla_w[-l] = np.dot(delta, activations[-l<span class="number">-1</span>].transpose())      <span class="comment"># delta 30*1, l=2, s0=x,784*1, ze w[-2]=w[0]=30*784 </span></div><div class="line"></div><div class="line">        <span class="keyword">return</span> (nabla_b, nabla_w)</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cost_derivative</span><span class="params">(self, output_activations, y)</span>:</span></div><div class="line"></div><div class="line">        <span class="string">"""Return the vector of partial derivatives \partial C_x /</span></div><div class="line"></div><div class="line">        \partial a for the output activations."""</div><div class="line"></div><div class="line">        <span class="keyword">return</span> (output_activations-y)</div></pre></td></tr></table></figure></p>
<p>　　完整代码请参考<a href="https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network.py" target="_blank" rel="external">书籍配套代码</a>。  </p>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="https://ziyubiti.github.io/2016/09/18/BPbrief/" data-id="clchhlk4e000w8npxpnzlttjy" class="article-share-link">Share</a><div class="tags"><a href="/tags/DeepLearning/">DeepLearning</a></div><div class="post-nav"><a href="/2016/09/22/lianjiaML/" class="pre">链家大数据使用到的机器学习算法</a><a href="/2016/09/08/deepdreamintro/" class="next">Deep Dream 初体验 - 神经网络模型眼中的世界</a></div><div data-thread-key="2016/09/18/BPbrief/" data-title="BP算法摘要与代码分析" data-url="https://ziyubiti.github.io/2016/09/18/BPbrief/" class="ds-share flat"><div class="ds-share-inline"><ul class="ds-share-icons-16"><li data-toggle="ds-share-icons-more"><a href="javascript:void(0);" class="ds-more">分享到：</a></li><li><a href="javascript:void(0);" data-service="weibo" class="ds-weibo">微博</a></li><li><a href="javascript:void(0);" data-service="qzone" class="ds-qzone">QQ空间</a></li><li><a href="javascript:void(0);" data-service="qqt" class="ds-qqt">腾讯微博</a></li><li><a href="javascript:void(0);" data-service="wechat" class="ds-wechat">微信</a></li></ul><div class="ds-share-icons-more"></div></div></div><div data-thread-key="2016/09/18/BPbrief/" data-title="BP算法摘要与代码分析" data-url="https://ziyubiti.github.io/2016/09/18/BPbrief/" data-author-key="1" class="ds-thread"></div></div></div></div><div class="pure-u-1-4"><div id="sidebar"><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/5G/">5G</a><span class="category-list-count">15</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Funny/">Funny</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Life/">Life</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/ML/">ML</a><span class="category-list-count">23</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Web/">Web</a><span class="category-list-count">3</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/AutoPilot/" style="font-size: 15px;">AutoPilot</a> <a href="/tags/Funny/" style="font-size: 15px;">Funny</a> <a href="/tags/DeepLearning/" style="font-size: 15px;">DeepLearning</a> <a href="/tags/Hexo/" style="font-size: 15px;">Hexo</a> <a href="/tags/LTE/" style="font-size: 15px;">LTE</a> <a href="/tags/NB-IoT/" style="font-size: 15px;">NB-IoT</a> <a href="/tags/5GNR/" style="font-size: 15px;">5GNR</a> <a href="/tags/MachineLearning/" style="font-size: 15px;">MachineLearning</a> <a href="/tags/Life/" style="font-size: 15px;">Life</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/crawl/" style="font-size: 15px;">crawl</a> <a href="/tags/Crab/" style="font-size: 15px;">Crab</a> <a href="/tags/RecSys/" style="font-size: 15px;">RecSys</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/07/16/HexoCommandNotFound/">Hexo：command not found的问题解决</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/08/08/olympic/">北京奥运十周年祭</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/07/25/hphone/">那些年买过的手机</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/07/11/autopilot/">科普佳作 -- 读《第一本无人驾驶技术书》</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/06/16/menggu/">蒙古各部的演化</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/18/nbiotpeakrate/">NB-IoT简介(4)--物理层上行与下行峰值速率的理论计算</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/11/nbiotsib/">NB-IoT简介(3)--SIB and Grid example</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/04/5gnewwave2/">5G下行候选新波形简介(2)--W-OFDM与F-OFDM</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/28/5gnrsssummary/">3GPP 5G-NR 随笔（1-5）：SSB/RMSI/BWP小结</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/21/hangyebijiao/">由个人博客不同文章阅读量，观察行业变迁与热点技术</a></li></ul></div><div class="widget"><div class="comments-title"><i class="fa fa-comment-o"> Recent Comments</i></div><div data-num-items="5" data-show-avatars="0" data-show-time="1" data-show-admin="0" data-excerpt-length="32" data-show-title="1" class="ds-recent-comments"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Blogroll</i></div><ul></ul><a href="http://blog.csdn.net/ziyubiti" title="ziyubiti's csdn blog" target="_blank">ziyubiti's csdn blog</a><ul></ul><a href="https://github.com/ziyubiti" title="ziyubiti's github repository" target="_blank">ziyubiti's github repository</a><ul></ul><a href="null" title="null" target="_blank"></a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">ziyubiti 2006-2023.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a> Theme by<a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> maupassant.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>var duoshuoQuery = {short_name:'ziyubiti'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>