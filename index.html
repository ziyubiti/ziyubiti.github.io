<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>ziyubiti | study hard everyday</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/4.2.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.0.0/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">ziyubiti</h1><a id="logo" href="/.">ziyubiti</a><p class="description">study hard everyday</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> Home</i></a><a href="/categories/ML/"><i class="fa undefined"> ML</i></a><a href="/categories/Python/"><i class="fa undefined"> Python</i></a><a href="/categories/Web/"><i class="fa undefined"> Web</i></a><a href="/categories/5G/"><i class="fa undefined"> 5G</i></a><a href="/categories/Funny/"><i class="fa undefined"> Funny</i></a><a href="/archives/"><i class="fa undefined"> Archive</i></a><a href="/about/"><i class="fa undefined"> About</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h2 class="post-title"><a href="/2016/11/01/actfunpaper/">神经网络不同激活函数比较--读《Understanding the difficulty of training deep feedforward neural networks》</a></h2><div class="post-meta">2016-11-01</div><a data-thread-key="2016/11/01/actfunpaper/" href="/2016/11/01/actfunpaper/#comments" class="ds-thread-count"></a><div class="post-content"><p>　　<img src="../../../../imgs/nnactfun/1.png" alt="various activation functions performance"><br>　　这篇论文比较了不同激活函数的中间各隐层输出值分布、梯度值分布、测试准确度等，从上图看出，至少在mnist和cifar10测试集下，看起来tanh和softsign都要好于sigmoid。</p></div><p class="readmore"><a href="/2016/11/01/actfunpaper/">Read More</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/10/24/nnpara/">神经网络参数优化</a></h2><div class="post-meta">2016-10-24</div><a data-thread-key="2016/10/24/nnpara/" href="/2016/10/24/nnpara/#comments" class="ds-thread-count"></a><div class="post-content"><h2 id="1-学习缓慢问题"><a href="#1-学习缓慢问题" class="headerlink" title=" 1.学习缓慢问题"></a> <strong>1.学习缓慢问题</strong></h2><p>　　激活函数采用sigmoid，代价函数采用二次项时，会存在学习缓慢即梯度下降较慢的问题。对此，可以使用交叉熵代价函数，或者使用ReLU、Softmax激活函数等，如下图。　　　　<br>　　<img src="../../../../imgs/nnpara/learningslow.png" alt="">  </p></div><p class="readmore"><a href="/2016/10/24/nnpara/">Read More</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/10/18/mlppaper/">回归初心-读《Deep Big Simple Neural Nets Excel on Hand-written Digit Recognition》</a></h2><div class="post-meta">2016-10-18</div><a data-thread-key="2016/10/18/mlppaper/" href="/2016/10/18/mlppaper/#comments" class="ds-thread-count"></a><div class="post-content"><p>　　Mnist做为手写数字识别的基准数据集，one Hidden Layer Perceptron已可以取得较好性能（Error rate 2%-5%），更有诸多新型网络结构如CNN、RNN不断改善性能（Error rate 1%以下）。<br>　　而这篇论文，则进一步回归初心，挖掘纯粹MLP的性能，通过可变学习速率，旋转、畸变数据以扩展训练集，更多隐藏层等，观察性能如下：<br>　　<img src="../../../../imgs/MLP/MLP.png" alt="MLP performance"><br>　　<br>　　没有花拳绣腿，纯粹的MLP，同样可以得到错误率0.5%以下的性能，唯一的缺点就是计算量，方法上简单、粗暴、有效！</p></div><p class="readmore"><a href="/2016/10/18/mlppaper/">Read More</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/10/11/datascientistway/">数据科学家路线图</a></h2><div class="post-meta">2016-10-11</div><a data-thread-key="2016/10/11/datascientistway/" href="/2016/10/11/datascientistway/#comments" class="ds-thread-count"></a><div class="post-content"><p>网上搜到的，感谢原作者。<br>　　<img src="../../../../imgs/datascientistway/1.jpg" alt="">  </p></div><p class="readmore"><a href="/2016/10/11/datascientistway/">Read More</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/10/08/lenetintro/">LeNet简介</a></h2><div class="post-meta">2016-10-08</div><a data-thread-key="2016/10/08/lenetintro/" href="/2016/10/08/lenetintro/#comments" class="ds-thread-count"></a><div class="post-content"><p>　　LeNet做为CNN的经典网络结构，如下。<br>　　<img src="../../../../imgs/lenet/lenet1.png" alt="LeNet-5架构"><br>　　<img src="../../../../imgs/lenet/lenet2.png" alt="S2到C3的映射"><br>　　<img src="../../../../imgs/lenet/lenet_para.png" alt="LeNet-5参数量">  </p></div><p class="readmore"><a href="/2016/10/08/lenetintro/">Read More</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/09/30/improvedlperformance/">如何改进深度学习的性能？</a></h2><div class="post-meta">2016-09-30</div><a data-thread-key="2016/09/30/improvedlperformance/" href="/2016/09/30/improvedlperformance/#comments" class="ds-thread-count"></a><div class="post-content"><p>摘自：<a href="http://machinelearningmastery.com/improve-deep-learning-performance/" target="_blank" rel="external">How To Improve Deep Learning Performance</a>  </p></div><p class="readmore"><a href="/2016/09/30/improvedlperformance/">Read More</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/09/22/lianjiaML/">链家大数据使用到的机器学习算法</a></h2><div class="post-meta">2016-09-22</div><a data-thread-key="2016/09/22/lianjiaML/" href="/2016/09/22/lianjiaML/#comments" class="ds-thread-count"></a><div class="post-content"><p>摘自: <a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2650994254&amp;idx=2&amp;sn=f591ab2d1a15a500fc1f12b6d55c52e9&amp;chksm=bdbf0e1d8ac8870be98a3f0eff315f2e9d74b079601eec23ed472f54cabc1310a13e7885a5b0&amp;scene=0#wechat_redirect" target="_blank" rel="external">InfoQ对链家网大数据架构师蔡白银的访谈</a>  </p></div><p class="readmore"><a href="/2016/09/22/lianjiaML/">Read More</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/09/18/BPbrief/">BP算法摘要与代码分析</a></h2><div class="post-meta">2016-09-18</div><a data-thread-key="2016/09/18/BPbrief/" href="/2016/09/18/BPbrief/#comments" class="ds-thread-count"></a><div class="post-content"><p>　　反向传播算法是神经网络中的重要组成部分，是对权重和偏置变化影响代价函数过程的理解，目标是计算代价函数C 分别关于w 和b的偏导数，从而更新w、b，影响前向过程，最小化代价函数。<br>　　<img src="../../../../imgs/BP/0.png" alt="神经网络结构示意图"><br>　　<img src="../../../../imgs/BP/1.png" alt="BP算法的4个方程"><br>　　公式BP1与BP2在nielsen的书<a href="http://neuralnetworksanddeeplearning.com/chap2.html" target="_blank" rel="external">Neural Networks and Deep Learning</a>中已有证明，下面补充BP3与BP4的证明：<br>　　<img src="../../../../imgs/BP/2.png" alt="BP3 and BP4 Proof"><br>　　BP算法过程如下：<br>　　<img src="../../../../imgs/BP/3.png" alt="BP算法过程"><br>　　在m大小的mini_batch内进行一次梯度更新，过程如下：<br>　　<img src="../../../../imgs/BP/4.png" alt="mini_batch内的一次梯度更新">    　　<br>　　<br>　　以mnist识别，三层网络[784,30,10]为例，nielsen的书中代码如下，添加了部分注释：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_mini_batch</span><span class="params">(self, mini_batch, eta)</span>:</span></div><div class="line"></div><div class="line">        <span class="string">"""Update the network's weights and biases by applying</span></div><div class="line"></div><div class="line">        gradient descent using backpropagation to a single mini batch.</div><div class="line"></div><div class="line">        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``</div><div class="line"></div><div class="line">        is the learning rate."""</div><div class="line"></div><div class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]    <span class="comment"># idx0:30*1; idx1:10*1</span></div><div class="line"></div><div class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]   <span class="comment"># idx0:30*784; idx1:10*30</span></div><div class="line"></div><div class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> mini_batch:                              <span class="comment"># loop 10, BP calc  and update gradient 10 times</span></div><div class="line"></div><div class="line">            delta_nabla_b, delta_nabla_w = self.backprop(x, y)        <span class="comment"># delta b,delta w , their dimension is same as b,w</span></div><div class="line"></div><div class="line">            nabla_b = [nb+dnb <span class="keyword">for</span> nb, dnb <span class="keyword">in</span> zip(nabla_b, delta_nabla_b)]  <span class="comment"># bj:zip two column vector, but list output row by row</span></div><div class="line"></div><div class="line">            nabla_w = [nw+dnw <span class="keyword">for</span> nw, dnw <span class="keyword">in</span> zip(nabla_w, delta_nabla_w)]</div><div class="line"></div><div class="line">        self.weights = [w-(eta/len(mini_batch))*nw</div><div class="line"></div><div class="line">                        <span class="keyword">for</span> w, nw <span class="keyword">in</span> zip(self.weights, nabla_w)]     <span class="comment"># update w, sum the average 10 changes</span></div><div class="line"></div><div class="line">        self.biases = [b-(eta/len(mini_batch))*nb</div><div class="line"></div><div class="line">                       <span class="keyword">for</span> b, nb <span class="keyword">in</span> zip(self.biases, nabla_b)]       <span class="comment"># update b,</span></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backprop</span><span class="params">(self, x, y)</span>:</span></div><div class="line"></div><div class="line">        <span class="string">"""Return a tuple ``(nabla_b, nabla_w)`` representing the</span></div><div class="line"></div><div class="line">        gradient for the cost function C_x.  ``nabla_b`` and</div><div class="line"></div><div class="line">        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar</div><div class="line"></div><div class="line">        to ``self.biases`` and ``self.weights``."""</div><div class="line"></div><div class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</div><div class="line"></div><div class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</div><div class="line"></div><div class="line">        <span class="comment"># feedforward</span></div><div class="line"></div><div class="line">        activation = x</div><div class="line"></div><div class="line">        activations = [x] <span class="comment"># list to store all the activations, layer by layer</span></div><div class="line"></div><div class="line">        zs = [] <span class="comment"># list to store all the z vectors, layer by layer</span></div><div class="line"></div><div class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</div><div class="line"></div><div class="line">            z = np.dot(w, activation)+b</div><div class="line"></div><div class="line">            zs.append(z)                              <span class="comment"># middle variable z before sigmoid, z1,z2</span></div><div class="line"></div><div class="line">            activation = sigmoid(z)</div><div class="line"></div><div class="line">            activations.append(activation)            <span class="comment"># activation output, x, s1,s2</span></div><div class="line"></div><div class="line">        <span class="comment"># backward pass</span></div><div class="line"></div><div class="line">        delta = self.cost_derivative(activations[<span class="number">-1</span>], y) * \                <span class="comment"># C=1/2 *(s2-y).^2, so dc/ds=(s2-y)</span></div><div class="line"></div><div class="line">            sigmoid_prime(zs[<span class="number">-1</span>])                                            <span class="comment"># delta 10*1</span></div><div class="line"></div><div class="line">        nabla_b[<span class="number">-1</span>] = delta</div><div class="line"></div><div class="line">        nabla_w[<span class="number">-1</span>] = np.dot(delta, activations[<span class="number">-2</span>].transpose())           <span class="comment"># s1: 30*1, s1':1*30; delta:10*1, dot:10*30</span></div><div class="line"></div><div class="line">        <span class="comment"># Note that the variable l in the loop below is used a little</span></div><div class="line"></div><div class="line">        <span class="comment"># differently to the notation in Chapter 2 of the book.  Here,</span></div><div class="line"></div><div class="line">        <span class="comment"># l = 1 means the last layer of neurons, l = 2 is the</span></div><div class="line"></div><div class="line">        <span class="comment"># second-last layer, and so on.  It's a renumbering of the</span></div><div class="line"></div><div class="line">        <span class="comment"># scheme in the book, used here to take advantage of the fact</span></div><div class="line"></div><div class="line">        <span class="comment"># that Python can use negative indices in lists.</span></div><div class="line"></div><div class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> xrange(<span class="number">2</span>, self.num_layers):</div><div class="line"></div><div class="line">            z = zs[-l]                                                         </div><div class="line"></div><div class="line">            sp = sigmoid_prime(z)                                           <span class="comment">#sp1: dz1, 30*1</span></div><div class="line"></div><div class="line">            delta = np.dot(self.weights[-l+<span class="number">1</span>].transpose(), delta) * sp       <span class="comment"># w[1]: 10*30;  delta update to 30*1 </span></div><div class="line"></div><div class="line">            nabla_b[-l] = delta</div><div class="line"></div><div class="line">            nabla_w[-l] = np.dot(delta, activations[-l<span class="number">-1</span>].transpose())      <span class="comment"># delta 30*1, l=2, s0=x,784*1, ze w[-2]=w[0]=30*784 </span></div><div class="line"></div><div class="line">        <span class="keyword">return</span> (nabla_b, nabla_w)</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cost_derivative</span><span class="params">(self, output_activations, y)</span>:</span></div><div class="line"></div><div class="line">        <span class="string">"""Return the vector of partial derivatives \partial C_x /</span></div><div class="line"></div><div class="line">        \partial a for the output activations."""</div><div class="line"></div><div class="line">        <span class="keyword">return</span> (output_activations-y)</div></pre></td></tr></table></figure></p></div><p class="readmore"><a href="/2016/09/18/BPbrief/">Read More</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/09/08/deepdreamintro/">Deep Dream 初体验 - 神经网络模型眼中的世界</a></h2><div class="post-meta">2016-09-08</div><a data-thread-key="2016/09/08/deepdreamintro/" href="/2016/09/08/deepdreamintro/#comments" class="ds-thread-count"></a><div class="post-content"><p>　　2015年年中Google发布了Deep Dream的新闻，展示了google神经网络模型对输入图片的理解，类似于“深度盗梦”，并引发了后续的艺术风格作画。<br>　　Deep Dream，其原理并不是尽可能地去正确识别图片对象，而是在网络识别对象模式的中间过程时，忽略确认操作，这样就解除了最小梯度限制，Test loss不再越来越小；而是让网络在已识别出的错误对象上，继续去强化认知和理解，最终找出了新图像的模板，而这迥异于原图的风格，形成了类似于创造梦境的效果。<br>　　Google的示例如下：<br>　　<img src="../../../../imgs/deepdream/googledeepdream.jpg" alt="google deep dream example 1"><br>　　<img src="../../../../imgs/deepdream/googledeepdream2.jpg" alt="google deep dream example 2 - 原图"><br>　　<img src="../../../../imgs/deepdream/googledeepdream3.jpg" alt="google deep dream example 2 - 转换图"><br>　　<img src="../../../../imgs/deepdream/googledeepdream4.jpg" alt="google deep dream example 3 - 原图"><br>　　<img src="../../../../imgs/deepdream/googledeepdream5.jpg" alt="google deep dream example 3 - 转换图">  　　</p></div><p class="readmore"><a href="/2016/09/08/deepdreamintro/">Read More</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/08/30/awesomeML/">机器学习相关的Awesome系列</a></h2><div class="post-meta">2016-08-30</div><a data-thread-key="2016/08/30/awesomeML/" href="/2016/08/30/awesomeML/#comments" class="ds-thread-count"></a><div class="post-content"><p></p><p>原文链接：<a href="https://www.52ml.net/17812.html" target="_blank" rel="nofollow">[原创]机器学习相关的Awesome系列</a></p><p></p></div><p class="readmore"><a href="/2016/08/30/awesomeML/">Read More</a></p></div><nav class="page-navigator"><span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/">Next</a></nav></div></div><div class="pure-u-1-4"><div id="sidebar"><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/5G/">5G</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Funny/">Funny</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Life/">Life</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/ML/">ML</a><span class="category-list-count">14</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Web/">Web</a><span class="category-list-count">2</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/crawl/" style="font-size: 15px;">crawl</a> <a href="/tags/Funny/" style="font-size: 15px;">Funny</a> <a href="/tags/NB-IoT/" style="font-size: 15px;">NB-IoT</a> <a href="/tags/LTE/" style="font-size: 15px;">LTE</a> <a href="/tags/MachineLearning/" style="font-size: 15px;">MachineLearning</a> <a href="/tags/DeepLearning/" style="font-size: 15px;">DeepLearning</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/Hexo/" style="font-size: 15px;">Hexo</a> <a href="/tags/Life/" style="font-size: 15px;">Life</a> <a href="/tags/Crab/" style="font-size: 15px;">Crab</a> <a href="/tags/RecSys/" style="font-size: 15px;">RecSys</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2016/11/01/actfunpaper/">神经网络不同激活函数比较--读《Understanding the difficulty of training deep feedforward neural networks》</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/24/nnpara/">神经网络参数优化</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/18/mlppaper/">回归初心-读《Deep Big Simple Neural Nets Excel on Hand-written Digit Recognition》</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/11/datascientistway/">数据科学家路线图</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/08/lenetintro/">LeNet简介</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/30/improvedlperformance/">如何改进深度学习的性能？</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/22/lianjiaML/">链家大数据使用到的机器学习算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/18/BPbrief/">BP算法摘要与代码分析</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/08/deepdreamintro/">Deep Dream 初体验 - 神经网络模型眼中的世界</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/08/30/awesomeML/">机器学习相关的Awesome系列</a></li></ul></div><div class="widget"><div class="comments-title"><i class="fa fa-comment-o"> Recent Comments</i></div><div data-num-items="5" data-show-avatars="0" data-show-time="1" data-show-admin="0" data-excerpt-length="32" data-show-title="1" class="ds-recent-comments"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Blogroll</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">ziyubiti 2006-2016.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a> Theme by<a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> maupassant.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>var duoshuoQuery = {short_name:'ziyubiti'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>